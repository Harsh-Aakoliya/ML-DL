# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16KjGJeGr-ouU5z7qxe9hJyey5UoUZzYj
"""

import torch
import torch.nn as nn
import pandas as pd

df=pd.read_csv("https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv")
df.head()

df["species"]=df["species"].replace("setosa",0.0)
df['species']=df['species'].replace("versicolor",1.0)
df['species']=df['species'].replace("virginica",2.0)

df

Y=df["species"]

X=df.drop("species",axis=1)

X

Y

print(type(X))
print(type(Y))
#hence why convert X and Y into numpy array

X=X.values
Y=Y.values

print(type(X))
print(type(Y))

#convert X and Y into tensors

X=torch.FloatTensor(X)
Y=torch.LongTensor(Y)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)

print(x_train)

print(x_test)

print(y_train)

print(y_test)

print(len(x_test),len(x_train),len(y_train),len(y_test))

import torch
from torch import nn
import torch.nn.functional as F

class Model(nn.Module):
  #input will be four features
  #output will be 3 clsses
  #and let there are two layers l1=8 neurons and l2=9 neurons so
  def __init__(self,inf=4,h1=8,h2=9,outf=3):
    super().__init__()
    self.fc1=nn.Linear(in_features=inf,out_features=h1)
    self.fc2=nn.Linear(in_features=h1,out_features=h2)
    self.fc3=nn.Linear(in_features=h2,out_features=outf)

  def forward(self,x):
    x=self.fc1(x)
    # x=nn.ReLU(x)
    x=F.relu(x)

    x=self.fc2(x)
    # x=nn.ReLU(x)
    x=F.relu(x)

    x=self.fc3(x) #this will be pradicted value

    return x

model=Model()

loss=nn.CrossEntropyLoss()
optim=torch.optim.Adam(model.parameters(),lr=0.01)

model.parameters

epochs=100
ithloss=[]
for i in range(epochs):
  y_pred=model.forward(x_train)
  lossi=loss(y_pred,y_train)
  ithloss.append(lossi.detach().numpy())

  if(i%10 == 0):
    print("epoch no",i,"Loss is ",lossi.detach().numpy())

  #now backpropogate in network to change the parameters
  optim.zero_grad()
  lossi.backward()
  optim.step()

with torch.no_grad():
  y_pred=model.forward(x_test)
  total_loss=loss(y_pred,y_test)
  print(total_loss)

#now to see what was the actual output and what model have preding
with torch.no_grad():
  for i,data in enumerate(x_test):
    predicted=model.forward(data)
    print("actual ",y_test[i].item(),"predicted ",predicted.argmax().item())





#CNN in pytorch

import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision import datasets,transforms

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

#to convert 2D MNIST images into 4D Tensors like (no of images,ht,wd,no of channels) we need transformer here we are using ToTensor
tranform=transforms.ToTensor()

training_data=datasets.MNIST(
    root="/cnn_data",
    train=True,
    download=True,
    transform=tranform
)

testing_data=datasets.MNIST(
    root="/cnn_data",
    train=False,
    download=True,
    transform=tranform
)

(training_data)

print(testing_data)

train_dataloader=DataLoader(training_data,batch_size=10,shuffle=True)
test_dataloader=DataLoader(testing_data,batch_size=10,shuffle=False)

print(train_dataloader)
print(test_dataloader)

# for i,(x_train,y_train) in enumerate(training_data):
#   print(x_train,y_train)
#   print((x_train).shape)
#   if(i==0):
#     break

# print(x_train,y_train)
# print(x_train.shape)

# torch.Size([1, 28, 28]) means we are getting one image of size 28X28

#so in x_train we have one image of size 28X28 and in y_train we have corrosponding label of that image

# #now let us define a convolution layer to visulize for one image

# conv1=nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3,stride=1,padding=0) #in_channels is no of images that we are providing
# conv2=nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3,stride=1,padding=0) #here in_channels will be out_channels of above layer

# #now to add batch for that image like 1 batch containg 1 image we need to do view
# x=x_train.view(1,1,28,28)

# print(x)

# #apply first convolution

# x=conv1(x)
# x=F.relu(x)

# print(x)

# #now we will be getting 6 as output as we have applied convolution so
# x.shape # here 1 is batch size, 6 is feature map and 26 is reduces size from 28 as we have applied convolution

#now apply second convolution

# x=conv2(x)
# x=F.relu(x)
# print(x)

# x.shape

# # now pass thru pooling layer
# x=F.max_pool2d(x,2,2)# first 2 is kernel_size and second 2 is stride
# x.shape





#network will be like conv->relu->maxpool->conv2->relu->maxpool->fc1->relu->fc2->relu->fc3->log_sigmoid
class Model(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1=nn.Conv2d(1,6,3,1) #in_channels,out_channels,kernel_size,stride,padding
    self.conv2=nn.Conv2d(6,16,3,1)

    self.fc1=nn.Linear(5*5*16,120)
    self.fc2=nn.Linear(120,84)
    self.fc3=nn.Linear(84,10)

  def forward(self,x):
    x=self.conv1(x)
    x=F.relu(x)
    x=F.max_pool2d(x,2,2)

    x=self.conv2(x)
    x=F.relu(x)
    x=F.max_pool2d(x,2,2)

    #now here x will be 2d so we need to convert it into 1x
    x=x.view(-1,16*5*5)

    #now we can apply fully connected
    x=self.fc1(x)
    x=F.relu(x)

    x=self.fc2(x)
    x=F.relu(x)

    x=self.fc3(x)

    return F.log_softmax(x,dim=1)

model=Model()
model

loss=nn.CrossEntropyLoss()
optim=torch.optim.Adam(model.parameters(),lr=0.1)

noofepoch=1
for i in range(noofepoch):
  #now for every epoch we need to train
  for batch,(x_train,y_train) in enumerate(train_dataloader):# batch is current batch number
    # print(x_train.shape,y_train.shape) # x_train will be [10,1,28,28] and y_train will be [10] means 10 numbers
    # for k in y_train:
    #   print(k)

    # print(x_traini)

    #this is also ok
    # for k in x_traini:
    #   y_predixted=model(k)
    #   print(y_predixted)

    #and this is also ok
    y_predicted=model(x_train) #it will return 10 predicted value one for each 10 images
    # print(y_predicted)

    loss_batch=loss(y_predicted,y_train)
    # print(y_predicted.shape,y_train.shape)
    # print(loss_batch)

    optim.zero_grad()
    loss_batch.backward()
    optim.step()

    if(batch % 600 ==0):
      print(batch,"->",loss_batch)

  #   break
  # break



















import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision import datasets
from torchvision.transforms import ToTensor
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

training_data=datasets.MNIST(
    root="/kaggle/temp/mnist_train",
    train=True,
    download=True
)

import matplotlib.pyplot as plt
print(((training_data[3][0])))
# plt.imshow(training_data[0][0].numpy().reshape(32, 32), cmap='gray')
# plt.show()

device=(
    "cude" if torch.cuda.is_available()
    else "cpu"
)
print(device)

train_dataloader=DataLoader(dataset=training_data,batch_size=64,shuffle=True)

print(train_dataloader)









import torch
from torch import nn
from torch.utils.data import dataloader
from torchvision import datasets, transforms

from torchvision.models import vgg19, VGG19_Weights

model=vgg19(VGG19_Weights)
model

device = (
    "cuda" if torch.cuda.is_available()
    else "cpu"
)

print(device)

model.to(device)

from PIL import Image
img=Image.open("/content/download.jpg")

#now we need to convert image in tensor
img

