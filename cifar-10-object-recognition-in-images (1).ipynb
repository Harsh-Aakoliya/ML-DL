{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-18T16:46:57.591117Z","iopub.status.busy":"2024-03-18T16:46:57.590707Z","iopub.status.idle":"2024-03-18T16:47:36.436800Z","shell.execute_reply":"2024-03-18T16:47:36.435798Z","shell.execute_reply.started":"2024-03-18T16:46:57.591087Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd # for reading csv file\n","\n","#train and test images are .7z archives, we need to unpack them\n","!pip install py7zr\n","\n","from py7zr import unpack_7zarchive\n","import shutil #sh utill\n","\n","#Before using, we need to register unpack format\n","shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n","\n","#unpack train images in /kaggle/working or /kaggle/temp\n","shutil.unpack_archive('/kaggle/input/cifar-10/train.7z', '/kaggle/temp/')#this temp directory is not visible to use explicitly\n","#here we are unzipping the files to \"/kaggle/temp/\" directory"]},{"cell_type":"markdown","metadata":{},"source":["#trainLabels.csv file have lebel of training data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#just checking the filenames\n","import os\n","i=0\n","for root, dirnames,fnames in os.walk(\"/kaggle/temp/train\"):\n","    for fname in sorted(fnames):\n","        print(fname)\n","        print(os.path.join(root,fname))\n","        i=i+1\n","        if(i==50):\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:05.049706Z","iopub.status.busy":"2024-03-18T16:49:05.048988Z","iopub.status.idle":"2024-03-18T16:49:06.870541Z","shell.execute_reply":"2024-03-18T16:49:06.869620Z","shell.execute_reply.started":"2024-03-18T16:49:05.049672Z"},"trusted":true},"outputs":[],"source":["import torch\n","if torch.cuda.is_available():\n","    device=torch.device(type=\"cuda\", index=0)\n","else:\n","    device=torch.device(type=\"cpu\", index=0) \n","    \n","    \n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:10.423093Z","iopub.status.busy":"2024-03-18T16:49:10.422500Z","iopub.status.idle":"2024-03-18T16:49:10.458013Z","shell.execute_reply":"2024-03-18T16:49:10.457002Z","shell.execute_reply.started":"2024-03-18T16:49:10.423056Z"},"trusted":true},"outputs":[],"source":["#first fetching the class names from trainLabels.csv\n","\n","train_labels=pd.read_csv(\"/kaggle/input/cifar-10/trainLabels.csv\", header='infer')\n","\n","#unique labels\n","classes=train_labels['label'].unique() # so we have total 50k images now to find all the unique classes \n","#we can apply unique() on \"label\" so classes will be all the 10 unique classes that this dataset have\n","\n","#confirming\n","print(classes)\n","\n","#classnames to classids\n","name2num={} #whenever we want to use numeric representation of classes then we can use this dictionary\n","i=0\n","for name in classes:\n","    name2num[name]=i\n","    i=i+1\n","\n","num2name={}#now in code we will be working with numerical values of class but at the time of submission \n","#we have to submit classes name so we can use this dictionary at that time\n","for i in range(len(classes)): #from 0 to 9\n","    num2name[i]=classes[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:12.885887Z","iopub.status.busy":"2024-03-18T16:49:12.885514Z","iopub.status.idle":"2024-03-18T16:49:14.451642Z","shell.execute_reply":"2024-03-18T16:49:14.450775Z","shell.execute_reply.started":"2024-03-18T16:49:12.885859Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset #because we are writing custom dataset and this module will inherite\n","#the Dataset\n","from torch.utils.data import DataLoader#dataloader to wrape around custom dataset\n","import os #because my images are there in directory like /kaggle/temp/train now to walk over it we have\n","#to import os\n","from torchvision.io import read_image #it will allow to read .png and .jpeg images and returs a tensor\n","from torchvision.transforms import ToTensor, Normalize, Resize, Compose#normalize we will be using z-score\n","#normalization and compose for to apply group of transformations and resize is for like our image is of\n","#32X32 for pretrained network like imageNET and it uses 224X224\n","\n","# normalization will work like we have 224X224X3 for all the 1M images so for all 1M images we have 3 channels\n","# so we will add all R values of 1M images (total 1M*224 R intensities) and divide by total R values which is \n","# nothing but 1M*224 \n","# and same for G and B\n","\n","# and from that we will be getting Rnorm=(Rvalue-(mean(R))/(SD(R))) so this normalization will be done by \n","# above imported normalize\n","\n","\n","#now we are creating our custom dataset named as TrainDataset and any custom dataset have three method\n","#first is init, len, getitem\n","#init-> at the time of creating instance of this dataset to initialize all the values we will use it\n","#len will return how many entries available for this dataset\n","#getiem will return {image,lable} pair one at a time\n","class TrainDataset(Dataset):\n","    def __init__(self, imgpath, labelpath):#imgpath will be \"/kaggle/temp/train\" and labelpath is #\"/kaggle/input/cifar-10/trainLabels.csv\"\n","        super().__init__()#we are calling base class constructor\n","        self.imgpath=imgpath\n","        self.labelpath=labelpath\n","        self.labels=pd.read_csv(labelpath, header='infer') #since labelpath is .csv file and it contains \n","        #two colums id and lable so without reading it's header we are storing id and lable (DF) into labels variable\n","        self.transform=Compose([Resize((224,224), antialias=True), Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n","        #so as we have imported that compose now we can use it here like\n","        #to use it we have to provide list of all the transformation seperated by \",\" so all the transformation will\n","        #be applied in sequence\n","#         1)as we have discussed above to change 32X32 to 223X224 we are applying resize transformation \n","#         2)so we are using z-score normalization so we need 3 means and 3 SDs (for R,G,B)\n","\n","    def __len__(self):\n","        return self.labels.shape[0] #we have 50,000 * 2 so it will return 50,000\n","    \n","    def __getitem__(self,idx):# so here one by one we are providing idx for which we need image,lable pair\n","        finalpath=os.path.join(self.imgpath,str(idx+1))+'.png'  #here idx will be from [0,49,999] but images are\n","        #from 1.png to 50000.png so we are finding that image path here \n","        # so for any exmaple if idx=0 then finalpath will be \"/kaggle/temp/train/1.png\"\n","        img=read_image(finalpath)/255 \n","        img=self.transform(img) #now we are applying that two transformer so from 32X32 it will resize to 224X224\n","        # and along with that all the R,G,B intensities will be normalized\n","        label=self.labels.iloc[idx,1] #now we have prepocessed image and we need it's label so self.labels\n","        # is dataframe so [idx,1] mean idx'th row and 1'st column because lable is on second column like labels is \n","#         DF having two columns [id,label] so here we are getting label in name form like \"cat\",\"aeroplae\" etc.\n","#and here we are not doing idx+1 because our header is inferd so for image1 label is at 0th row , for image2 label is at 1st row \n","# and we are providing idx also in form of 0,1,2,......\n","        label=name2num[label]#we are converting this label into numeric form because for computation.\n","        return img,label #here img is tensor because read_image already convers image to tensor\n","\n","traindataset=TrainDataset('/kaggle/temp/train','/kaggle/input/cifar-10/trainLabels.csv')        \n","\n","batch_size=64    \n","traindataloader=DataLoader(dataset=traindataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:14.693174Z","iopub.status.busy":"2024-03-18T16:49:14.692793Z","iopub.status.idle":"2024-03-18T16:49:14.700645Z","shell.execute_reply":"2024-03-18T16:49:14.699668Z","shell.execute_reply.started":"2024-03-18T16:49:14.693144Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n","#here we are using one of the popular CNN mobilenet_v3_large and it's weight as MobileNet_V3_Large_Weights\n","\n","#now we are creating NN and any NN have atleast two methods first is init and second is forward\n","#in init method we are initializing all the layers (they are not connected only initialized)\n","class Cifar10Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pretrainednet=mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT) #.DEFAULT mean import best \n","        #available MobileNet_V3_Large_Weights so pretraindnet is now referring to mobilenet_v3_large\n","        #mobilenet have 3 childrens 1)features 2)avgpool and 3) classifier \n","        #here at the last layer outfeatures are 1000 means it is trained for 1000 classes\n","        #so self.pretrainednet is entire network and have 3 chiles as above and self.pretrainednet.classifier\n","#         is the  last child and it has to be changed because it have total 1000 neuron but we only need 10\n","#         classes so it is totally mobilenet network but we are only changing the last layer 1000 -> 10 classes\n","\n","        #means  self.pretrainednet have same features and avgpool but have different classifier and it is below\n","        self.pretrainednet.classifier=nn.Sequential(\n","            nn.Linear(in_features=960, out_features=1280, \n","                   bias=True),nn.Hardswish(), \n","            nn.Dropout(p=0.2, inplace=True), \n","            nn.Linear(in_features=1280, out_features=10, \n","                      bias=True)\n","        )\n","        #since outfeature is 10 so we will return 10 classed probability one for each\n","    \n","    \n","    def forward(self,x):#x will be 64 pairs of 224 X 224 X 3 image and 1 corrosponding lable\n","        x=self.pretrainednet(x) # so here x will become 64 X 10 because at last we have out_features as 10\n","        return x "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:20.154900Z","iopub.status.busy":"2024-03-18T16:49:20.154043Z","iopub.status.idle":"2024-03-18T16:49:20.163635Z","shell.execute_reply":"2024-03-18T16:49:20.162529Z","shell.execute_reply.started":"2024-03-18T16:49:20.154867Z"},"trusted":true},"outputs":[],"source":["\n","def train_one_epoch(dataloader, model,loss_fn, optimizer):\n","    model.train()# we are telling model that now you will be in train mode \n","    track_loss=0\n","    num_correct=0\n","    num_param=0\n","    \n","    for i, (imgs, labels) in enumerate(dataloader):\n","        imgs=imgs.to(device)\n","        labels=labels.to(device)\n","        pred=model(imgs)\n","                    \n","        loss=loss_fn(pred,labels)\n","        track_loss+=loss.item()\n","        num_correct+=(torch.argmax(pred,dim=1)==labels).type(torch.float).sum().item()\n","        \n","        running_loss=round(track_loss/(i+(imgs.shape[0]/batch_size)),2)\n","        running_acc=round((num_correct/((i*batch_size+imgs.shape[0])))*100,2)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if i%100==0:\n","            print(\"Batch:\", i+1, \"/\",len(dataloader), \"Running Loss:\",running_loss, \"Running Accuracy:\",running_acc)\n","            \n","    epoch_loss=running_loss\n","    epoch_acc=running_acc\n","    return epoch_loss, epoch_acc"]},{"cell_type":"markdown","metadata":{},"source":["avgpool do not have any wts and biases so there is not to do anything with it\n","#goal \n","1) backpropogate only at classifier i.e. at the last layers of network means parametes of features not be changed so require_grad will be false here for features\n","2) backpropogate to entire network so here require grad of features will be true \n","\n","\n","1)\n","for param in model.pretrainednet.features.parameters():\n","    param.requires_grad=True\n","    \n"," \n"," here model is is instance of cifar10Net and pretrainednet is reffering to mobile_net_v3 with changed classifier and it have three childs features, avgpool and classifier so model.pretrainednet.features have so many parameters  so one by one it will go in param variable \n"," \n"," and param.requires_grad=False means these parameter will not be updated during backpropogation and it is only for features not for all the layers in network means we are only backpropogating into classifier that mean all the weights and baises of classifier will be changed accoring to earlier layers in network so we brought classifier's wts and biases to that level that earliear layers have\n"," \n"," \n"," 2)\n"," for param in model.pretrainednet.features.parameters():\n","    param.requires_grad=True"]},{"cell_type":"markdown","metadata":{},"source":["loss_fn=nn.CrossEntropyLoss() #here we are using loss function as crossentropyloss\n","lr=0.001\n","#optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\n","optimizer=torch.optim.Adam(params=model.parameters(), lr=lr) #here parms will be all the parameters but we already specified that requires_grad =False for features so it will not be updated\n","n_epochs=30\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T16:49:22.196265Z","iopub.status.busy":"2024-03-18T16:49:22.195892Z","iopub.status.idle":"2024-03-18T18:26:11.419485Z","shell.execute_reply":"2024-03-18T18:26:11.418329Z","shell.execute_reply.started":"2024-03-18T16:49:22.196227Z"},"trusted":true},"outputs":[],"source":["model=Cifar10Net()\n","model=model.to(device)\n","\n","for param in model.pretrainednet.features.parameters():\n","    param.requires_grad=False\n","\n","loss_fn=nn.CrossEntropyLoss()\n","lr=0.001\n","#optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\n","optimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\n","n_epochs=30\n","\n","for i in range(n_epochs):\n","    print(\"Epoch No:\",i+1)\n","    train_epoch_loss, train_epoch_acc=train_one_epoch(traindataloader,model,loss_fn,optimizer)\n","    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n","    print(\"--------------------------------------------------\")\n","\n","for param in model.pretrainednet.features.parameters():\n","    param.requires_grad=True\n","\n","for i in range(n_epochs):\n","    print(\"Epoch No:\",i+1)\n","    train_epoch_loss, train_epoch_acc=train_one_epoch(traindataloader,model,loss_fn,optimizer)\n","    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n","    print(\"--------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T18:27:19.797468Z","iopub.status.busy":"2024-03-18T18:27:19.797003Z","iopub.status.idle":"2024-03-18T18:30:28.939921Z","shell.execute_reply":"2024-03-18T18:30:28.939020Z","shell.execute_reply.started":"2024-03-18T18:27:19.797433Z"},"trusted":true},"outputs":[],"source":["#unpacking test images, there are 3 lacs images. This will take some time\n","shutil.unpack_archive('/kaggle/input/cifar-10/test.7z', '/kaggle/temp/') # here length of test is 3lakhs\n","#so sampleSubmission.csv file will be having 3lakhs entries\n","\n","#unregister unpack format, we are done with it\n","shutil.unregister_unpack_format('7zip')#, ['.7z'], unpack_7zarchive)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T18:30:53.348837Z","iopub.status.busy":"2024-03-18T18:30:53.348451Z","iopub.status.idle":"2024-03-18T18:30:53.689075Z","shell.execute_reply":"2024-03-18T18:30:53.688295Z","shell.execute_reply.started":"2024-03-18T18:30:53.348804Z"},"trusted":true},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, imgpath):#imgpath will be \"kaggle/temp/test\"\n","        super().__init__()\n","        self.imgpath=imgpath \n","        _,_,self.files=next(os.walk(self.imgpath)) # next(os.walk(self.imgpath)) will return 3 things \n","#         1) path to root directory i.e. kaggle/temp/test\n","#         2) list of all the subdirectories\n","#         3) list of all the files that it contains\n","#         so _,_,self.files means we are not interested into first two thing i.e. path to root directory and \n","#         list of all the subdirectory we only need all the images i.e. 3rd thing so we are storing it into\n","#         files variable like files will have list of all the 3 lakhs images path\n","        self.length=len(self.files)# it will be 3lakhs\n","        self.transform=Compose([Resize((224,224), antialias=True), Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])        \n","        #we are providing same tranformer as traing \n","    def __len__(self):\n","        return self.length\n","    \n","    def __getitem__(self,idx):\n","        finalpath=os.path.join(self.imgpath,str(idx+1))+'.png'\n","        img=read_image(finalpath)/255.0\n","        img=self.transform(img)\n","        return img #it will only return image because it is testing dataset and it don't have lable for it\n","\n","testdataset=TestDataset('/kaggle/temp/test/')\n","testdataloader=DataLoader(dataset=testdataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T18:31:18.259164Z","iopub.status.busy":"2024-03-18T18:31:18.258797Z","iopub.status.idle":"2024-03-18T18:31:18.267341Z","shell.execute_reply":"2024-03-18T18:31:18.266355Z","shell.execute_reply.started":"2024-03-18T18:31:18.259138Z"},"trusted":true},"outputs":[],"source":["def eval(dataloader, model,loss_fn, path):\n","    model.eval()\n","    data=pd.read_csv(path) #data\n","    with torch.no_grad():#for efficiency purpose because if we don't want to backpropogate the why should we have to track of it\n","        for i, imgs in enumerate(dataloader): #i will be from 0 to 63 imgs will be 224 X 224 X 3\n","            finalbatchpred=np.zeros(imgs.shape[0],dtype='object') # imgs.shape[0] will be 64 here dtype is object because \n","            #because altimetly we will be storing answer in word formate like \"cat\", \"dog\", \"aeroplane\" etc.\n","#             means  finalbatchpred will be 64 0's but type having as object \n","            imgs=imgs.to(device)\n","            pred=model(imgs)#pred will be of 64 X 10\n","            \n","            pred=torch.argmax(pred,dim=1).type(torch.int).cpu() #now pred will be 64 X 1 and all 64 value will be from 0 to 9\n","            for j,p in enumerate(pred):\n","                finalbatchpred[j]=num2name[p.item()] #since p is tensor so p.item() will be in numeric value mean we are unpacking value from tensor\n","            data.iloc[i*batch_size:i*batch_size+batch_size ,1]=finalbatchpred #chanign default predicton (\"cat\") to predicted value\n","    \n","    data.to_csv('submission.csv', index=False) #so it will be stored into /kaggle/working/submission.csv\n","    data.head()"]},{"cell_type":"markdown","metadata":{},"source":["so first 50 prediction will be \n","id,label\n","1,deer\n","2,deer\n","3,automobile\n","4,ship\n","5,airplane\n","6,cat\n","7,airplane\n","8,dog\n","9,ship\n","10,cat\n","11,bird\n","12,horse\n","13,frog\n","14,deer\n","15,dog\n","16,airplane\n","17,dog\n","18,bird\n","19,airplane\n","20,deer\n","21,ship\n","22,ship\n","23,ship\n","24,automobile\n","25,frog\n","26,truck\n","27,automobile\n","28,cat\n","29,cat\n","30,cat\n","31,cat\n","32,horse\n","33,dog\n","34,airplane\n","35,deer\n","36,cat\n","37,cat\n","38,horse\n","39,ship\n","40,airplane\n","41,bird\n","42,bird\n","43,frog\n","44,deer\n","45,horse\n","46,automobile\n","47,deer\n","48,bird\n","49,airplane"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T18:31:27.316552Z","iopub.status.busy":"2024-03-18T18:31:27.316151Z","iopub.status.idle":"2024-03-18T18:39:26.526860Z","shell.execute_reply":"2024-03-18T18:39:26.525589Z","shell.execute_reply.started":"2024-03-18T18:31:27.316522Z"},"trusted":true},"outputs":[],"source":["eval(testdataloader, model,loss_fn, '/kaggle/input/cifar-10/sampleSubmission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":46718,"sourceId":3649,"sourceType":"competition"},{"datasetId":4595564,"sourceId":7839372,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
