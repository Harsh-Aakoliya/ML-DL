{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torchvision.transforms import ToTensor\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T02:58:58.460139Z","iopub.execute_input":"2024-04-29T02:58:58.460856Z","iopub.status.idle":"2024-04-29T02:58:58.465777Z","shell.execute_reply.started":"2024-04-29T02:58:58.460823Z","shell.execute_reply":"2024-04-29T02:58:58.464789Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(type='cuda',index=0)\nelse:\n    device=torch.device(type='cpu',index=0)\n\nprint(f'Using device {device}')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:00.658845Z","iopub.execute_input":"2024-04-29T02:59:00.659764Z","iopub.status.idle":"2024-04-29T02:59:00.666152Z","shell.execute_reply.started":"2024-04-29T02:59:00.659728Z","shell.execute_reply":"2024-04-29T02:59:00.665115Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using device cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=True,\n                            download=True,\n                            transform=ToTensor())\n\neval_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=False,\n                            download=True,\n                            transform=ToTensor())\n\nbatch_size=64\n\ntrain_dataloader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader=DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:02.689374Z","iopub.execute_input":"2024-04-29T02:59:02.690364Z","iopub.status.idle":"2024-04-29T02:59:02.777175Z","shell.execute_reply.started":"2024-04-29T02:59:02.690333Z","shell.execute_reply":"2024-04-29T02:59:02.776377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Encoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=784,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=256)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.lkrelu(net2)\n        return out2\n    \nclass Decoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.sig=nn.Sigmoid()\n        self.l1=nn.Linear(in_features=256,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=784)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.sig(net2)\n        return out2\n\nclass Encoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=256,out_features=100)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n    \nclass Decoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=100,out_features=256)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.out=nn.Linear(in_features=100,out_features=10)\n    \n    def forward(self,x):\n        out=self.out(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:05.733838Z","iopub.execute_input":"2024-04-29T02:59:05.734523Z","iopub.status.idle":"2024-04-29T02:59:05.749498Z","shell.execute_reply.started":"2024-04-29T02:59:05.734491Z","shell.execute_reply":"2024-04-29T02:59:05.748529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lossmse_fn=nn.MSELoss()\nlossentropy_fn=nn.CrossEntropyLoss()\nlr=0.001\n\ne1=Encoder1().to(device)\nd1=Decoder1().to(device)\ne2=Encoder2().to(device)\nd2=Decoder2().to(device)\nclf=Classifier().to(device)\n\nopte1=Adam(params=e1.parameters(),lr=lr)\noptd1=Adam(params=d1.parameters(),lr=lr)\nopte2=Adam(params=e2.parameters(),lr=lr)\noptd2=Adam(params=d2.parameters(),lr=lr)\noptclf=Adam(params=clf.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:09.357976Z","iopub.execute_input":"2024-04-29T02:59:09.358649Z","iopub.status.idle":"2024-04-29T02:59:09.528857Z","shell.execute_reply.started":"2024-04-29T02:59:09.358611Z","shell.execute_reply":"2024-04-29T02:59:09.527864Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#train AutoEncoder1\ndef train_e1d1():    \n    track_loss=0\n    \n    e1.train()\n    d1.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latent=e1(x)\n        pred=d1(latent)\n            \n        loss=lossmse_fn(pred,x)\n        \n        track_loss+=loss.item()\n          \n        loss.backward()\n        \n        opte1.step()\n        optd1.step()\n        \n        opte1.zero_grad()\n        optd1.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    return round(running_loss,4)\n\n#train AutoEncoder2\ndef train_e2d2():    \n    track_loss=0\n    \n    e2.train()\n    d2.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1.detach())\n        pred=d2(latente2)\n            \n        loss=lossmse_fn(pred,latente1.detach()) \n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte2.step()\n        optd2.step()\n        opte2.zero_grad()\n        optd2.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n\n#train Classifier\ndef train_clf():  \n    track_loss=0\n    num_correct=0\n    \n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2.detach())\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        optclf.step()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#finetune whole \ndef train_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.train()\n    e2.train()\n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2)\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte1.step()\n        opte2.step()\n        optclf.step()\n        \n        opte1.zero_grad()\n        opte2.zero_grad()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4),round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:11.973773Z","iopub.execute_input":"2024-04-29T02:59:11.974396Z","iopub.status.idle":"2024-04-29T02:59:11.994167Z","shell.execute_reply.started":"2024-04-29T02:59:11.974366Z","shell.execute_reply":"2024-04-29T02:59:11.993239Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Eval AutoEncoder1\ndef eval_e1d1():\n    track_loss=0\n    \n    e1.eval()\n    d1.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latent=e1(x)\n            pred=d1(latent)\n\n            loss=lossmse_fn(pred,x)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4),x,pred\n\n#Eval AutoEncoder2\ndef eval_e2d2():\n    track_loss=0\n    \n    e2.eval()\n    d2.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=d2(latente2)\n\n            loss=lossmse_fn(pred,latente1)\n            \n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n            \n        \n#Eval Classifier\ndef eval_clf():\n    track_loss=0\n    num_correct=0\n    \n    clf.eval()\n    \n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#eval whole \ndef eval_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.eval()\n    e2.eval()\n    clf.eval()\n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4), round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:16.840580Z","iopub.execute_input":"2024-04-29T02:59:16.840947Z","iopub.status.idle":"2024-04-29T02:59:16.858579Z","shell.execute_reply.started":"2024-04-29T02:59:16.840920Z","shell.execute_reply":"2024-04-29T02:59:16.857712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"n_epochs=20\n\nprint(\"-----------------------Auto Encoder 1------------------------\")\nfor i in range(n_epochs):\n    train_loss=train_e1d1()\n    eval_loss,x,pred=eval_e1d1()\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_loss,\", Eval Loss=\",eval_loss,sep=\"\")\n    \nplt.figure(figsize=(3.2,2.4))\n\nr=torch.randint(low=0,high=pred.shape[0],size=(1,)).item()\n\nplt.subplot(1,2,1)\nplt.title(\"Orignal\")\nplt.imshow(torch.reshape(x[r],shape=(28,28)).cpu())\n\nplt.subplot(1,2,2)\nplt.title(\"Prediction\")\nplt.imshow(torch.reshape(pred[r],shape=(28,28)).cpu())\nplt.show()\n    \nprint(\"-----------------------Auto Encoder 2------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_e2d2(),\", Eval Loss=\",eval_e2d2(),sep=\"\") \n\n\nprint(\"-----------------------Classifier Only------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_clf(),\", Eval Loss & Accuracy=\",eval_clf(),sep=\"\") \n\n\nprint(\"--------------------Fine-Tuning Whole Metwork------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_whole(),\", Eval Loss & Accuracy=\",eval_whole(),sep=\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T02:59:20.959386Z","iopub.execute_input":"2024-04-29T02:59:20.959838Z","iopub.status.idle":"2024-04-29T03:11:54.666626Z","shell.execute_reply.started":"2024-04-29T02:59:20.959805Z","shell.execute_reply":"2024-04-29T03:11:54.665571Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"-----------------------Auto Encoder 1------------------------\nEpoch=1, Train Loss=0.0221, Eval Loss=0.0083\nEpoch=2, Train Loss=0.0067, Eval Loss=0.0055\nEpoch=3, Train Loss=0.0049, Eval Loss=0.0043\nEpoch=4, Train Loss=0.0039, Eval Loss=0.0037\nEpoch=5, Train Loss=0.0034, Eval Loss=0.0034\nEpoch=6, Train Loss=0.0031, Eval Loss=0.0032\nEpoch=7, Train Loss=0.0028, Eval Loss=0.0028\nEpoch=8, Train Loss=0.0026, Eval Loss=0.0026\nEpoch=9, Train Loss=0.0024, Eval Loss=0.0025\nEpoch=10, Train Loss=0.0023, Eval Loss=0.0024\nEpoch=11, Train Loss=0.0021, Eval Loss=0.0023\nEpoch=12, Train Loss=0.0021, Eval Loss=0.0022\nEpoch=13, Train Loss=0.002, Eval Loss=0.0021\nEpoch=14, Train Loss=0.0019, Eval Loss=0.002\nEpoch=15, Train Loss=0.0018, Eval Loss=0.002\nEpoch=16, Train Loss=0.0018, Eval Loss=0.0019\nEpoch=17, Train Loss=0.0017, Eval Loss=0.0019\nEpoch=18, Train Loss=0.0016, Eval Loss=0.0018\nEpoch=19, Train Loss=0.0016, Eval Loss=0.0019\nEpoch=20, Train Loss=0.0016, Eval Loss=0.0018\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 320x240 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAScAAACyCAYAAAAAhgkFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAguUlEQVR4nO3de1xUdfoH8M8MMwwgMNzvIGjeVrstJaKoWKwkqXmpVjMTtcTEC7rlL1030/W3vFZLaV1Tu4iVKaa/zDS1lBAvi7qirusFUkOlFESTAQG5zff3hzH1nEGGgYE5A8/79ZrXy+ecM+d8Hb48nPPM93yPQgghwBhjMqO0dgMYY6w+nJwYY7LEyYkxJkucnBhjssTJiTEmS5ycGGOyxMmJMSZLnJwYY7LEyYkxJkucnCzsrbfegkKhsHYzDKKjoxEdHW3tZrDfCA0NRXx8vCHev38/FAoF9u/fb7FjKBQKvPXWWxbbnzVwcvqNs2fP4sUXX0RgYCA0Gg0CAgIwbtw4nD171tpNYxa0fv16KBQKw8vBwQFdu3bF9OnTUVhYaO3mNdquXbtsPgE1RGXtBsjFF198gbFjx8LDwwOTJ09GWFgYLl++jI8++ghbt25FWloaRo4caXI/CxYswBtvvNEKLWbNtXjxYoSFheHu3bs4dOgQVq9ejV27duHMmTNwcnJqtXYMGDAAFRUVsLe3N+t9u3btwqpVq+pNUBUVFVCpbPvX27ZbbyGXLl3C+PHj0alTJxw4cADe3t6GdbNmzUL//v0xfvx4nD59Gp06dap3H2VlZejQoQNUKpXNd4r2YsiQIXjssccAAC+//DI8PT2xfPlybN++HWPHjjXavu5nbGlKpRIODg4W3ael92cNfFkHYNmyZSgvL8f7779PEhMAeHl5Ye3atSgrK8PSpUsB/FpXOnfuHF544QW4u7sjKiqKrPutiooKzJw5E15eXnBxccHw4cPx008/GdUF6t578eJFxMfHw83NDVqtFhMnTkR5eTnZZ2pqKp544gn4+PhAo9Hgd7/7HVavXt0Cn0778cQTTwAA8vLyEB8fD2dnZ1y6dAlxcXFwcXHBuHHjAAB6vR4pKSno2bMnHBwc4Ovri4SEBNy+fZvsTwiBJUuWICgoCE5OThg0aFC9JYL71ZyOHj2KuLg4uLu7o0OHDnjooYfw7rvvAgDi4+OxatUqACCXqHXqqzmdPHkSQ4YMgaurK5ydnfHkk0/iyJEjZJu6S97Dhw9jzpw58Pb2RocOHTBy5EgUFRWZ/6E2A/+JB7Bjxw6Ehoaif//+9a4fMGAAQkND8fXXX5Plzz33HLp06YK//e1vaGjmmfj4eHz++ecYP348+vTpg8zMTDz99NP33f75559HWFgYkpOTceLECXz44Yfw8fHB3//+d8M2q1evRs+ePTF8+HCoVCrs2LED06ZNg16vR2JiopmfAAPunUEDgKenJwCgpqYGsbGxiIqKwttvv2241EtISMD69esxceJEzJw5E3l5efjnP/+JkydP4vDhw1Cr1QCAN998E0uWLEFcXBzi4uJw4sQJDB48GFVVVSbbsnfvXgwdOhT+/v6YNWsW/Pz8cP78eezcuROzZs1CQkICrl27hr179+LTTz81ub+zZ8+if//+cHV1xdy5c6FWq7F27VpER0cjMzMTERERZPsZM2bA3d0dCxcuxOXLl5GSkoLp06dj8+bNZn2mzSLaueLiYgFAPPPMMw1uN3z4cAFAlJSUiIULFwoAYuzYsUbb1a2rk52dLQCIpKQksl18fLwAIBYuXGj03kmTJpFtR44cKTw9Pcmy8vJyo2PHxsaKTp06kWUDBw4UAwcObPD/1t6kpqYKAGLfvn2iqKhI5Ofni7S0NOHp6SkcHR3Fjz/+KCZMmCAAiDfeeIO89+DBgwKA+Oyzz8jyPXv2kOU3btwQ9vb24umnnxZ6vd6w3fz58wUAMWHCBMOyjIwMAUBkZGQIIYSoqakRYWFhomPHjuL27dvkOL/dV2Jiorjfr7C0b40YMULY29uLS5cuGZZdu3ZNuLi4iAEDBhh9NjExMeRYs2fPFnZ2dqK4uLje47WEdn9ZV1paCgBwcXFpcLu69SUlJYZlU6dONbn/PXv2AACmTZtGls+YMeO+75Hut3///rh16xY5tqOjo+HfOp0ON2/exMCBA/HDDz9Ap9OZbBcDYmJi4O3tjeDgYIwZMwbOzs7Ytm0bAgMDDdu8+uqr5D1btmyBVqvFH/7wB9y8edPwCg8Ph7OzMzIyMgAA+/btQ1VVFWbMmEEut5KSkky26+TJk8jLy0NSUhLc3NzIuqYMU6mtrcW3336LESNGkJqpv78/XnjhBRw6dIj0LQCYMmUKOVb//v1RW1uLK1eumH38pmr3l3V1SacuSd1PfUksLCzM5P6vXLkCpVJptO0DDzxw3/eEhISQ2N3dHQBw+/ZtuLq6AgAOHz6MhQsXIisry6gepdPpoNVqTbatvVu1ahW6du0KlUoFX19fdOvWDUrlr3+vVSoVgoKCyHsuXLgAnU4HHx+fevd548YNADD8Enfp0oWs9/b2Nvw876fu8rJXr17m/Yfuo6ioCOXl5ejWrZvRuh49ekCv1yM/Px89e/Y0LG+oD7aWdp+ctFot/P39cfr06Qa3O336NAIDAw3JAaBnL5ZkZ2dX73LxS13r0qVLePLJJ9G9e3csX74cwcHBsLe3x65du7BixQro9foWaVdb07t3b8O3dfXRaDQkWQH3iuE+Pj747LPP6n2P9AsVW2WqD7aGdp+cAGDo0KH44IMPcOjQIcO3br918OBBXL58GQkJCWbvu2PHjtDr9cjLyyN/RS9evNjk9u7YsQOVlZX46quvyF+4uksK1nI6d+6Mffv2oV+/fg3+cerYsSOAe2dav72UKioqMnn20blzZwDAmTNnEBMTc9/tGnuJ5+3tDScnJ+Tm5hqty8nJgVKpRHBwcKP21Zrafc0JAF5//XU4OjoiISEBt27dIut+/vlnTJ06FU5OTnj99dfN3ndsbCwA4L333iPLV65c2eT21v1V++1fMZ1Oh9TU1CbvkzXO888/j9raWvz1r381WldTU4Pi4mIA9+pZarUaK1euJD+nlJQUk8f4/e9/j7CwMKSkpBj2V+e3+6obcyXdRsrOzg6DBw/G9u3bcfnyZcPywsJCbNy4EVFRUeSKQC74zAn36gIff/wxxo0bhwcffNBohPjNmzexadMmw180c4SHh2P06NFISUnBrVu3DEMJvv/+ewBNK3AOHjwY9vb2GDZsGBISEnDnzh188MEH8PHxwfXr183eH2u8gQMHIiEhAcnJyTh16hQGDx4MtVqNCxcuYMuWLXj33Xfx7LPPwtvbG6+99hqSk5MxdOhQxMXF4eTJk9i9eze8vLwaPIZSqcTq1asxbNgwPPLII5g4cSL8/f2Rk5ODs2fP4ptvvgFwr28BwMyZMxEbGws7OzuMGTOm3n0uWbIEe/fuRVRUFKZNmwaVSoW1a9eisrLSMH5Pbjg5/eK5555D9+7dkZycbEhInp6eGDRoEObPn9+s4uQnn3wCPz8/bNq0Cdu2bUNMTAw2b96Mbt26NWkkb7du3bB161YsWLAAr732Gvz8/PDqq6/C29sbkyZNanI7WeOsWbMG4eHhWLt2LebPnw+VSoXQ0FC8+OKL6Nevn2G7JUuWwMHBAWvWrEFGRgYiIiLw7bffNjjGrU5sbCwyMjKwaNEivPPOO9Dr9ejcuTNeeeUVwzajRo3CjBkzkJaWhg0bNkAIcd/k1LNnTxw8eBDz5s1DcnIy9Ho9IiIisGHDBqMxTnKhEK1Z4WIGp06dwqOPPooNGzYYRh4zxn7FNadWUFFRYbQsJSUFSqUSAwYMsEKLGJM/vqxrBUuXLkV2djYGDRoElUqF3bt3Y/fu3ZgyZYosvyVhTA74sq4V7N27F4sWLcK5c+dw584dhISEYPz48fjzn//MMxgwdh+cnBhjssQ1J8aYLLVYclq1ahVCQ0Ph4OCAiIgIHDt2rKUOxWwM9w3WGC1yWbd582a89NJLWLNmDSIiIpCSkoItW7YgNzf3vjdM1tHr9bh27RpcXFxk9aAA1nhCCJSWliIgIMDo3rTm9A2A+4eta6hv1LexxfXu3VskJiYa4traWhEQECCSk5NNvjc/P18A4FcbeOXn51u0b3D/aDuv+vqGlMW/KqqqqkJ2djbmzZtnWKZUKhETE4OsrCyj7SsrK1FZWWmIxS8nclGIgwpqSzePtYIaVOMQdhnNkWVu3wC4f7Q19+sb9bF4crp58yZqa2vh6+tLlvv6+iInJ8do++TkZCxatKiehqmhUnDns0n38ofRZZe5fQPg/tHm3Kdv1Mfq39bNmzcPOp3O8MrPz7d2k5iMcP9ovyx+5uTl5QU7OzujhxMWFhbCz8/PaHuNRgONRmPpZjAZMrdvANw/2jOLnznZ29sjPDwc6enphmV6vR7p6emIjIy09OGYDeG+wczRIvdOzJkzBxMmTMBjjz2G3r17IyUlBWVlZZg4cWJLHI7ZEO4brLFaJDn98Y9/RFFREd58800UFBTgkUcewZ49e4wKoaz94b5hIdKCchu8C01299aVlJRAq9UiGs/wtzE2qkZUYz+2Q6fTWXz6V+4fv7DR5GRO37D6t3WMMVYfTk6MMVniyYRkonBGXxL/+w36dJY1xZ1I/OGHdB5qvxX/apmGMfOZGmCokJwTCPqcQWU9j5yqiO5J4tf/8SmJ+2p+JvH2slASfz4qmsT673+gTaipuV9rrYbPnBhjssTJiTEmS5ycGGOyxDUnmeg19hyJq0UtiSdrL9D4Tykknvo8fWz1D+/0IHGHrUeb2ULWaNKv9aU1KD392UJpR0Ot8VfsQ/6+n8SDHctIrFY4kXisy08k9vtqO4mXv0Sfb6c8dpbEolbSRqlWGLrAZ06MMVni5MQYkyVOTowxWeKakxVcXNHHaNlEz83N2udHIRkk/vx/aQ3hk6388E6rMVGfUXag9aKcuaFG26S6/R+J9bAncbXkELWSY0Y56Ehcsv5LEn/8B/rk6Zor1p83i8+cGGOyxMmJMSZLnJwYY7LUJmtOd4f2JrFjQTmJxfEzLXr8qqceJ/HNBDomJaf3KqP3SMc1SZ2vpvGya0+RuJ/bJRK/6HqexEsW/pHEIYv4XrwmM3O6EoWa1ocUPeh9kgHv/0jijYHLjfbhrHQgsbS/XKypIvEnt+nMosO1J0jc14GOg5qf7EnizuNom6wxJQufOTHGZImTE2NMljg5McZkqU3WnBx2HiNxS18t/zyRXt9/9tbbJA6yk04nawdzvbwsicQ+79Ga0Yez6fxOk/9E78WrdqVzBjEzmJyfia5X2NGfb01ULxLP/mAjiQc6FJNYozB+FJYe9OcnrUH+z8REEquP0YeUfrGQrk8fu4zEooDWtOQw7S+fOTHGZImTE2NMljg5McZkqU3WnFpayVh6b5zjmAISh6kk1+8SaoVxzen/7niROLVbRxL7oOFxSX3GnjR5DGYhknqMQkV/jZTdOpM44G8XSRzlcJtuL6lBnq+WFJQAzL74PInV87QkVp08TWK9ZD4mu7u0LuahpG0eNvA4bYMMHj3FZ06MMVni5MQYkyVOTowxWeKaUxN0nkHHkLwf8i2JpXPrSD169CWjZT5r6LPK1DhutE1Djmx6lLbhT5kkXjR0C4k3ruxH4prLV806XrsmmfPbLtCfxJcX01+rTcHfkNhZMo5JLxmJ99ZVOmYNADTT6Fg5/Q/03knpnN/SsVZVbnSclEbyKPeFPgdIPO6BF0lcezGPNojnEGeMtVecnBhjsmR2cjpw4ACGDRuGgIAAKBQKfPnll2S9EAJvvvkm/P394ejoiJiYGFy4cKH+nbE25bYowilxGP/CHgDAzp07yXruG8wcZtecysrK8PDDD2PSpEkYNWqU0fqlS5fiH//4Bz7++GOEhYXhL3/5C2JjY3Hu3Dk4ODQ8/kcupPfK/fwQvb5u7nzfgaPOmt7IwkZ2uE7iz5ws/7OoRQ2coYUvgnAW/zZabzN9Q3qvnIrWZ+x8vUmckxRI4k8efY/E0hpTDWh96K9Fvyfx3ckuRk3SX6Zzehs9V87oWXn0vMPjv/T/dGdUJYk1CpoKCmJ8SeyTR2uSoqbGqI2WZnZyGjJkCIYMGVLvOiEEUlJSsGDBAjzzzDMAgE8++QS+vr748ssvMWbMmHrfx9oGL4U/vOCPGmE8iJD7BjOXRWtOeXl5KCgoQEzMr0+f1Wq1iIiIQFZWVr3vqaysRElJCXmxtqcpfQPg/tGeWTQ5FRTcu43D15eeEvr6+hrWSSUnJ0Or1RpewcH8CKO2qCl9A+D+0Z5ZfZzTvHnzMGfOHENcUlLS6h3QzteHxN4vXSHxv7ruILGp+b4fPfwyiTsvuivZ4nvzGtgCbi6jNQp346E1smCN/iEdI2QX6EfigpW0PvbVgytI/ICa/lpJqzPRp+klrMcrtH/oCyXzdwMQNZJLZRPjjKQ1KfdcegylpK7mpKTznLuPpnOMK1Lp/6k1ak4WPXPy87v3QywsLCTLCwsLDeukNBoNXF1dyYu1PU3pGwD3j/bMoskpLCwMfn5+SE9PNywrKSnB0aNHERkZ2cA7WVvHfYOZy+zLujt37uDixV+ngMjLy8OpU6fg4eGBkJAQJCUlYcmSJejSpYvh6+KAgACMGDHCku1mMlQjalCBO6j55ULmypUr3DdYk5mdnI4fP45BgwYZ4rp6wIQJE7B+/XrMnTsXZWVlmDJlCoqLixEVFYU9e/bIaxyLRGnfMBJ/02WlZAvz5kaS1phqz1m/xiTVye0WiW/fZztzlOBnnMCv92jNnz8f8+fPt7m+odDQcUmXJtFxTF8/ROffDlU5kdhOMsYordSdxO4zab2otoBe6gp9I+5bMzHfkkJS95JeI5XraU3KWUHvvYvyps9B/LcjrcvirrSOanlmJ6fo6GiIBopxCoUCixcvxuLFi5vVMGZ7PBQ+iMGzqBHV2I/t0Ol0pEbEfYOZg++tY4zJEicnxpgsWX2ckzVcXEHnAM95fpVkC1pjks7HfbWmgsRjF75OYvdz9x/x3FpMzSGubPGn+dmuysjuJP56Aq0xhajo3FvS+Zj+W0nvW3s/kd6Dan/1DD2gZFwV9PWMITJ3/iRJ3aroIdpmJ2XD/cNLdUdyfDrHvamalyXwmRNjTJY4OTHGZImTE2NMltpFzUmhpvcN6V3oNb2pe+WkNabR78wlse/6hp8pZw2m/k96KBpc354oO3Qg8ch395HY1Dimw3fpGKF5c2aQ2CnzFD2g0EtC0eD6eklrPlKSfaju0mOoTYzd81UX0wWSsV/SOa5EdVXD7WkCPnNijMkSJyfGmCxxcmKMyRInJ8aYLLWLgviVPz9G4rNPpZj1fukgSzkWwFnTFUx4mMQvuu6VbEFvTL5aQwcozlhK+4fP7hMkNpoozpT6BjRKC+CSorxCKVkvGdhZ7kPX25koqB8t7UwXaOlDFxSl9DPggjhjrN3g5MQYkyVOTowxWWoXNafwwefM2v73HyWRuKMN1JhKH2h40KXUAx2KSHwyrBOJa/LoQx7aEumg3P+ZtYnETko6wLBC0HrKU+/TQbghqdkkFpIbf40bYOKmWVMDLAFAMlmckAyqVPnTB396DaAPVVWaOC95Qkt/Z87b0RqUqLJ8jUmKz5wYY7LEyYkxJkucnBhjstQuak6mrCmmk4uFfUGn+2/EbZgtTvH4gyS+QsseyO37HomrTcz95aEqI7He1ek+W7Y9SsmYHQ87OmZHJanffFhC63Ed3/0vifWmakwtQTJZnCrQn8Q5s4NIvK/72/TtaPhm5qIa+nxARYXk/6ho+fMaPnNijMkSJyfGmCxxcmKMyVK7qDkpJQ8MlE7+r1ZIxgg1ZpyJhdm50wcv/hTfg8TScUzSGpOpBxpIHdd1JLH+P+fNer8tU6jpOKZSvfSBBfSBkS5KOtmgwo+OIVJU0PWi1rwxZ0btkz7wAIDSTUviinD6INihy+n9gBtcPyexq7LhGpN0csJ1V/qR2OVuOYnNvl+wCfjMiTEmS5ycGGOyxMmJMSZL7aLm9P1tHxJXB9Pr68naCySu/YzWnE6UhJD44ru/I7Hbf+m4KFy7QeMAenyvD+l9TgDgqqLX9Fv9U4y2+S1T45ikNYTzkhLB1eVdSdwBRxveYRtSW3STxAv+8wyJh/ddT+KxLoUk7vA1fQDC67tfIPEDabQGpf7xFonFXTpmSPh6kjh3rvGYs+WRtIb0sP12EgdJHvSplMxBJVUpaIf4j+RWObsV9CGaNQX/bnB/LYHPnBhjsmRWckpOTsbjjz8OFxcX+Pj4YMSIEcjNzSXb3L17F4mJifD09ISzszNGjx6NwsLC++yRtRV5IgfHRDoyxJc4jN0AgAsX6Bkp9w1mDrOSU2ZmJhITE3HkyBHs3bsX1dXVGDx4MMrKfr0VYvbs2dixYwe2bNmCzMxMXLt2DaNGjWpgr6wtKEYRgtAZj2MQHkZfAMDIkSO5b7AmUwhR34TFjVNUVAQfHx9kZmZiwIAB0Ol08Pb2xsaNG/Hss88CAHJyctCjRw9kZWWhT58+JvdZUlICrVaLaDwDlUJtcvvGsOvZjcRbv/nUrPdLxxCZemDl4DNjSPxtr7QG9wcAp6voPpdde4rE6zruMauNvQ5NJLHvJlqDcPzyWIP7a44aUY39uFcTsWTfABrZP0zNlyQh7R+rdn1E4jC1M4lrJQ+srBT0Ia0Xa+j6dbeiSByooTXKoc70Xr0H1PQBlgCgN3GHp/R+QGmbUkvofExvH6T9q9NmyUM495+SNKB5Y7fq1PUNnU4HV1fXBrdtVs1Jp9MBADw8PAAA2dnZqK6uRkxMjGGb7t27IyQkBFlZWfXuo7KyEiUlJeTF2o7m9A2A+0d71uTkpNfrkZSUhH79+qFXr14AgIKCAtjb28PNzY1s6+vri4KCgnr3k5ycDK1Wa3gFBwc3tUlMJgTunan06dOnWX0D4P7RnjU5OSUmJuLMmTNIS0szvXED5s2bB51OZ3jl5+c3a3/M+i7gNABg3bp1zd4X94/2q0njnKZPn46dO3fiwIEDCAr6dd4YPz8/VFVVobi4mPyFLCwshJ+fX7370mg00GiMr7EtSf99HonD308i8ehRB0k834vOCW0uaY2pMcavSyLxxkkrSHy+mtZRXl5Gt/ffT8fudLpyicT6Mjp/U0vJESdxC/fOhAIDAw3Lm9I3gCb2DzPLqLXnL5J44tTZJF686gMSh0rGpHko6a9RD8m9e8v86BgyJejPUg/6/9Pp6b19AHC0ko6F+qnag8SB6p9JnHSM1j27zqVzxncrOElio/sBm16KthizzpyEEJg+fTq2bduG7777DmFh9ObD8PBwqNVqpKenG5bl5ubi6tWriIyMtEyLmSwJIZAjTqIIP+Fh9DNaz32DmcusM6fExERs3LgR27dvh4uLi6FWoNVq4ejoCK1Wi8mTJ2POnDnw8PCAq6srZsyYgcjIyEZ/G8NsUy5OogD5eBh9YfdLtyosLIRarea+wZrErOS0evVqAEB0dDRZnpqaivj4eADAihUroFQqMXr0aFRWViI2NhbvvfceWNv2I34AAGQj07Csa9eu3DdYkzVrnFNLaIlxTqaowujcRj8OD6QbSIbNTHxlF4mnaL8363iRbycZLQvIoGNfrkXT+Z06FNBxKC6bj5h1zNZkzlgWc7VK/5CMk1KF0Pm473am90rmjaB/4+MiT5H4Tz7pJFZL+tP2O3Turg/WDDNqkt+/JEMoJG3U20vmFD9HnztYe1ty/6eVtNo4J8YYaymcnBhjssTJiTEmS1xzYhZn8zWnZlKo7UmsdKT3NYoaet+bqJbEjZmfW16/to3GNSfGmM3j5MQYkyVOTowxWWoXc4gz1ppENZ2Qu1YSs8bhMyfGmCxxcmKMyRInJ8aYLHFyYozJEicnxpgscXJijMkSJyfGmCxxcmKMyRInJ8aYLHFyYozJEicnxpgscXJijMkSJyfGmCxxcmKMyZLspkypmzW4BtWAbc5E2u7V4N40sy0xAzT3D9tmTt+QXXIqLS0FABzCLhNbMrkrLS2FVqu1+D4B7h+2rjF9Q3YPONDr9bh27RqEEAgJCUF+fr7FJ8lvL0pKShAcHNzqn6EQAqWlpQgICIBSadnKAfcPy7CFviG7MyelUomgoCCUlNx7wqmrqyt3vmayxmdo6TOmOtw/LEvOfYML4owxWeLkxBiTJdkmJ41Gg4ULF0Kj0Vi7KTarLX+Gbfn/1hps4fOTXUGcMcYAGZ85McbaN05OjDFZ4uTEGJMlTk6MMVni5MQYkyXZJqdVq1YhNDQUDg4OiIiIwLFjx6zdJFlKTk7G448/DhcXF/j4+GDEiBHIzc0l29y9exeJiYnw9PSEs7MzRo8ejcLCQiu1uPm4bzSOzfcNIUNpaWnC3t5erFu3Tpw9e1a88sorws3NTRQWFlq7abITGxsrUlNTxZkzZ8SpU6dEXFycCAkJEXfu3DFsM3XqVBEcHCzS09PF8ePHRZ8+fUTfvn2t2Oqm477ReLbeN2SZnHr37i0SExMNcW1trQgICBDJyclWbJVtuHHjhgAgMjMzhRBCFBcXC7VaLbZs2WLY5vz58wKAyMrKslYzm4z7RtPZWt+Q3WVdVVUVsrOzERMTY1imVCoRExODrKwsK7bMNuh0OgCAh4cHACA7OxvV1dXk8+zevTtCQkJs7vPkvtE8ttY3ZJecbt68idraWvj6+pLlvr6+KCgosFKrbINer0dSUhL69euHXr16AQAKCgpgb28PNzc3sq0tfp7cN5rOFvuG7KZMYU2XmJiIM2fO4NChQ9ZuCpMZW+wbsjtz8vLygp2dndE3BoWFhfDz87NSq+Rv+vTp2LlzJzIyMhAUFGRY7ufnh6qqKhQXF5PtbfHz5L7RNLbaN2SXnOzt7REeHo709HTDMr1ej/T0dERGRlqxZfIkhMD06dOxbds2fPfddwgLCyPrw8PDoVaryeeZm5uLq1ev2tznyX3DPDbfN6xdka9PWlqa0Gg0Yv369eLcuXNiypQpws3NTRQUFFi7abLz6quvCq1WK/bv3y+uX79ueJWXlxu2mTp1qggJCRHfffedOH78uIiMjBSRkZFWbHXTcd9oPFvvG7JMTkIIsXLlShESEiLs7e1F7969xZEjR6zdJFnCvWeQGL1SU1MN21RUVIhp06YJd3d34eTkJEaOHCmuX79uvUY3E/eNxrH1vsHzOTHGZEl2NSfGGAM4OTHGZIqTE2NMljg5McZkiZMTY0yWODkxxmSJkxNjTJY4OTHGZImTE2NMljg5McZkiZMTY0yW/h+2yooIdLYKPgAAAABJRU5ErkJggg=="},"metadata":{}},{"name":"stdout","text":"-----------------------Auto Encoder 2------------------------\nEpoch=1, Train Loss=0.2644, Eval Loss=0.0895\nEpoch=2, Train Loss=0.0684, Eval Loss=0.0542\nEpoch=3, Train Loss=0.0528, Eval Loss=0.0493\nEpoch=4, Train Loss=0.0501, Eval Loss=0.0482\nEpoch=5, Train Loss=0.0492, Eval Loss=0.0469\nEpoch=6, Train Loss=0.0487, Eval Loss=0.0459\nEpoch=7, Train Loss=0.0484, Eval Loss=0.0468\nEpoch=8, Train Loss=0.0482, Eval Loss=0.0469\nEpoch=9, Train Loss=0.0481, Eval Loss=0.0465\nEpoch=10, Train Loss=0.048, Eval Loss=0.047\nEpoch=11, Train Loss=0.0479, Eval Loss=0.0453\nEpoch=12, Train Loss=0.0479, Eval Loss=0.0463\nEpoch=13, Train Loss=0.0477, Eval Loss=0.0455\nEpoch=14, Train Loss=0.0477, Eval Loss=0.0456\nEpoch=15, Train Loss=0.0477, Eval Loss=0.0457\nEpoch=16, Train Loss=0.0476, Eval Loss=0.0458\nEpoch=17, Train Loss=0.0476, Eval Loss=0.0464\nEpoch=18, Train Loss=0.0476, Eval Loss=0.0454\nEpoch=19, Train Loss=0.0476, Eval Loss=0.0457\nEpoch=20, Train Loss=0.0475, Eval Loss=0.0459\n-----------------------Classifier Only------------------------\nEpoch=1, Train Loss & Accuracy=(1.0092, 74.99), Eval Loss & Accuracy=(0.5508, 87.78)\nEpoch=2, Train Loss & Accuracy=(0.49, 87.74), Eval Loss & Accuracy=(0.4103, 89.66)\nEpoch=3, Train Loss & Accuracy=(0.4077, 89.05), Eval Loss & Accuracy=(0.3637, 90.32)\nEpoch=4, Train Loss & Accuracy=(0.3719, 89.72), Eval Loss & Accuracy=(0.3492, 90.17)\nEpoch=5, Train Loss & Accuracy=(0.3529, 90.06), Eval Loss & Accuracy=(0.3285, 90.97)\nEpoch=6, Train Loss & Accuracy=(0.3409, 90.35), Eval Loss & Accuracy=(0.318, 91.2)\nEpoch=7, Train Loss & Accuracy=(0.3331, 90.55), Eval Loss & Accuracy=(0.3132, 91.04)\nEpoch=8, Train Loss & Accuracy=(0.3275, 90.66), Eval Loss & Accuracy=(0.3146, 91.28)\nEpoch=9, Train Loss & Accuracy=(0.3224, 90.78), Eval Loss & Accuracy=(0.3128, 91.0)\nEpoch=10, Train Loss & Accuracy=(0.3191, 90.82), Eval Loss & Accuracy=(0.3009, 91.55)\nEpoch=11, Train Loss & Accuracy=(0.3162, 90.88), Eval Loss & Accuracy=(0.3019, 91.31)\nEpoch=12, Train Loss & Accuracy=(0.3144, 90.88), Eval Loss & Accuracy=(0.3025, 91.19)\nEpoch=13, Train Loss & Accuracy=(0.313, 90.98), Eval Loss & Accuracy=(0.3015, 91.62)\nEpoch=14, Train Loss & Accuracy=(0.3116, 91.01), Eval Loss & Accuracy=(0.2994, 91.53)\nEpoch=15, Train Loss & Accuracy=(0.3104, 91.07), Eval Loss & Accuracy=(0.3006, 91.3)\nEpoch=16, Train Loss & Accuracy=(0.3089, 91.09), Eval Loss & Accuracy=(0.2999, 91.46)\nEpoch=17, Train Loss & Accuracy=(0.3081, 91.07), Eval Loss & Accuracy=(0.3008, 91.49)\nEpoch=18, Train Loss & Accuracy=(0.3075, 91.09), Eval Loss & Accuracy=(0.3068, 91.42)\nEpoch=19, Train Loss & Accuracy=(0.3066, 91.2), Eval Loss & Accuracy=(0.2979, 91.53)\nEpoch=20, Train Loss & Accuracy=(0.3059, 91.22), Eval Loss & Accuracy=(0.2981, 91.51)\n--------------------Fine-Tuning Whole Metwork------------------\nEpoch=1, Train Loss & Accuracy=(0.2476, 93.14), Eval Loss & Accuracy=(0.1171, 96.58)\nEpoch=2, Train Loss & Accuracy=(0.0949, 97.03), Eval Loss & Accuracy=(0.1001, 97.08)\nEpoch=3, Train Loss & Accuracy=(0.0659, 97.93), Eval Loss & Accuracy=(0.0852, 97.46)\nEpoch=4, Train Loss & Accuracy=(0.0518, 98.41), Eval Loss & Accuracy=(0.095, 97.32)\nEpoch=5, Train Loss & Accuracy=(0.042, 98.59), Eval Loss & Accuracy=(0.0861, 97.65)\nEpoch=6, Train Loss & Accuracy=(0.0366, 98.87), Eval Loss & Accuracy=(0.0898, 97.47)\nEpoch=7, Train Loss & Accuracy=(0.0311, 98.99), Eval Loss & Accuracy=(0.0792, 98.0)\nEpoch=8, Train Loss & Accuracy=(0.0299, 99.0), Eval Loss & Accuracy=(0.096, 97.81)\nEpoch=9, Train Loss & Accuracy=(0.0266, 99.08), Eval Loss & Accuracy=(0.0963, 97.82)\nEpoch=10, Train Loss & Accuracy=(0.0241, 99.23), Eval Loss & Accuracy=(0.1012, 97.93)\nEpoch=11, Train Loss & Accuracy=(0.026, 99.2), Eval Loss & Accuracy=(0.1176, 97.84)\nEpoch=12, Train Loss & Accuracy=(0.023, 99.26), Eval Loss & Accuracy=(0.1101, 97.98)\nEpoch=13, Train Loss & Accuracy=(0.0175, 99.45), Eval Loss & Accuracy=(0.1072, 97.84)\nEpoch=14, Train Loss & Accuracy=(0.0211, 99.32), Eval Loss & Accuracy=(0.1121, 97.72)\nEpoch=15, Train Loss & Accuracy=(0.0178, 99.46), Eval Loss & Accuracy=(0.1233, 97.74)\nEpoch=16, Train Loss & Accuracy=(0.0171, 99.5), Eval Loss & Accuracy=(0.1566, 97.46)\nEpoch=17, Train Loss & Accuracy=(0.0159, 99.52), Eval Loss & Accuracy=(0.1228, 97.8)\nEpoch=18, Train Loss & Accuracy=(0.0202, 99.41), Eval Loss & Accuracy=(0.1246, 97.76)\nEpoch=19, Train Loss & Accuracy=(0.0146, 99.56), Eval Loss & Accuracy=(0.1061, 98.24)\nEpoch=20, Train Loss & Accuracy=(0.0159, 99.5), Eval Loss & Accuracy=(0.121, 98.0)\n","output_type":"stream"}]}]}