{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trTB3Bm7mW7K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.models import vgg19, VGG19_Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg19(VGG19_Weights.DEFAULT)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5U0iRE5nWCD",
        "outputId": "8aba01ce-8cee-4b11-80a9-02240c2eaec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:04<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(self, encoder, center, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.center = center\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = self.encoder(x)\n",
        "        center_output = self.center(encoder_outputs[-1])\n",
        "        decoder_output = self.decoder(center_output, encoder_outputs)\n",
        "\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "GweyV55joThj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, pretrained_network):\n",
        "        super().__init__()\n",
        "        self.encoder = pretrained_network\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "\n",
        "        for layers in self.encoder.features:\n",
        "            x = layers(x)\n",
        "            encoder_outputs.append(x)\n",
        "\n",
        "        return encoder_outputs"
      ],
      "metadata": {
        "id": "yvpid0gw5U94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Center(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = (3,3), padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(1024)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = (3,3), padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "hV_gPJoK5Xea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv5_up = nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.conv5_1 = nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn5_1 = nn.BatchNorm2d(512)\n",
        "        self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn5_2 = nn.BatchNorm2d(512)\n",
        "        self.conv5_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn5_3 = nn.BatchNorm2d(512)\n",
        "        self.conv5_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn5_4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv4_up = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.conv4_1 = nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn4_1 = nn.BatchNorm2d(512)\n",
        "        self.conv4_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn4_2 = nn.BatchNorm2d(512)\n",
        "        self.conv4_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn4_3 = nn.BatchNorm2d(512)\n",
        "        self.conv4_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3,3), padding = 1)\n",
        "        self.bn4_4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv3_up = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = (3,3), padding = 1)\n",
        "        self.conv3_1 = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = (3,3), padding = 1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(256)\n",
        "        self.conv3_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3,3), padding = 1)\n",
        "        self.bn3_2 = nn.BatchNorm2d(256)\n",
        "        self.conv3_3 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3,3), padding = 1)\n",
        "        self.bn3_3 = nn.BatchNorm2d(256)\n",
        "        self.conv3_4 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3,3), padding = 1)\n",
        "        self.bn3_4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv2_up = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = (3,3), padding = 1)\n",
        "        self.conv2_1 = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = (3,3), padding = 1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(128)\n",
        "        self.conv2_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3,3), padding = 1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv1_up = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = (3,3), padding = 1)\n",
        "        self.conv1_1 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = (3,3), padding = 1)\n",
        "        self.bn1_1 = nn.BatchNorm2d(64)\n",
        "        self.conv1_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 1)\n",
        "        self.bn1_2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv_final = nn.Conv2d(in_channels = 64, out_channels = 2, kernel_size = (1,1))\n",
        "\n",
        "    def forward(self, x, encoder_outputs):\n",
        "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "        x = self.conv5_up(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat((x, encoder_outputs[51]), dim = 1)\n",
        "        x = self.conv5_1(x)\n",
        "        x = self.bn5_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv5_2(x)\n",
        "        x = self.bn5_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv5_3(x)\n",
        "        x = self.bn5_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv5_4(x)\n",
        "        x = self.bn5_4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "        x = self.conv4_up(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat((x, encoder_outputs[38]), dim = 1)\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.bn4_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.bn4_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_3(x)\n",
        "        x = self.bn4_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_4(x)\n",
        "        x = self.bn4_4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "        x = self.conv3_up(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat((x, encoder_outputs[25]), dim = 1)\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.bn3_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.bn3_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_3(x)\n",
        "        x = self.bn3_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_4(x)\n",
        "        x = self.bn3_4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "        x = self.conv2_up(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat((x, encoder_outputs[12]), dim = 1)\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.bn2_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.bn2_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor = 2, mode = 'nearest')\n",
        "        x = self.conv1_up(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat((x, encoder_outputs[5]), dim = 1)\n",
        "        x = self.conv1_1(x)\n",
        "        x = self.bn1_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.bn1_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv_final(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z5NkbwZw5bZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(type=\"cuda\", index=0)\n",
        "else:\n",
        "    device=torch.device(type=\"cpu\", index=0)"
      ],
      "metadata": {
        "id": "o8Uz3JTz5oMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('/kaggle/input/sai-vessel-segmentation2/all/train/34_training.tif')\n",
        "mask = Image.open('/kaggle/input/sai-vessel-segmentation2/all/train/34_manual1.gif')\n",
        "\n",
        "print(\"Image Type:\", type(img))\n",
        "print(\"Mask Type:\", type(mask))\n",
        "print(f\"Image Shape: {np.array(img).shape} | Mask Sahpe: {np.array(mask).shape}\")\n",
        "\n",
        "print(\"Unique values in Mask:\",np.unique(mask))\n",
        "print(\"Image Data Type:\", np.array(img).dtype)\n",
        "print(\"Mask Data Type:\", np.array(mask).dtype)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(img)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Bzjsiim5pEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape(path):\n",
        "    original_image = Image.open(path)\n",
        "    reshaped_image = original_image.resize((512,512), PIL.Image.NEAREST)\n",
        "    return reshaped_image"
      ],
      "metadata": {
        "id": "UilqRUYQ59TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = reshape('/kaggle/input/sai-vessel-segmentation2/all/train/34_training.tif')\n",
        "mask = reshape('/kaggle/input/sai-vessel-segmentation2/all/train/34_manual1.gif')\n",
        "\n",
        "print(\"Image Type:\", type(img))\n",
        "print(\"Mask Type:\", type(mask))\n",
        "print(f\"Image Shape: {np.array(img).shape} | Mask Sahpe: {np.array(mask).shape}\")\n",
        "\n",
        "print(\"Unique values in Mask:\",np.unique(mask))\n",
        "print(\"Image Data Type:\", np.array(img).dtype)\n",
        "print(\"Mask Data Type:\", np.array(mask).dtype)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(img)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gFMd0ugs5-Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        _,_,self.files = next(os.walk(path))\n",
        "        self.length = int(len(self.files)/2) - 4\n",
        "        self.transform = Compose([ToTensor(), Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx + 21\n",
        "        path = self.path + str(idx) + \"_training.tif\"\n",
        "        img = reshape(path)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        path = self.path + str(idx) + \"_manual1.gif\"\n",
        "        mask = reshape(path)\n",
        "        mask = np.array(mask)\n",
        "        mask = torch.from_numpy(mask).type(torch.long)\n",
        "        mask[mask == 255] = 1\n",
        "\n",
        "        return img,mask\n",
        "\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        _,_,self.files = next(os.walk(path))\n",
        "        self.length = int(len(self.files)/2) - 16\n",
        "        self.transform = Compose([ToTensor(), Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx + 37\n",
        "        path = self.path + str(idx) + \"_training.tif\"\n",
        "        img = reshape(path)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        path = self.path + str(idx) + \"_manual1.gif\"\n",
        "        mask = reshape(path)\n",
        "        mask = np.array(mask)\n",
        "        mask = torch.from_numpy(mask).type(torch.long)\n",
        "        mask[mask == 255] = 1\n",
        "\n",
        "        return img,mask"
      ],
      "metadata": {
        "id": "GSITaIdS6GO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrainDataset(\"/kaggle/input/sai-vessel-segmentation2/all/train/\")\n",
        "validation_dataset = ValidationDataset(\"/kaggle/input/sai-vessel-segmentation2/all/train/\")\n",
        "\n",
        "batch_size = 4\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size)"
      ],
      "metadata": {
        "id": "GERY2U0j6JGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,mask=train_dataset[13]\n",
        "\n",
        "print(\"Image Shape:\",img.shape,\"Mask Shape:\",mask.shape,\"Image dtype:\",img.dtype, \"Mask dtype:\", mask.dtype)\n",
        "print(\"Unique values in Mask:\",np.unique(mask))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original Image 512 x 512 from Train_Dataset\")\n",
        "plt.imshow(torch.permute(img,(1,2,0)))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Mask 512 x 512 from Train_Dataset\")\n",
        "plt.imshow(mask)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bEHeLF7l6L07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    track_loss = 0\n",
        "    XintY = 0\n",
        "    X = 0\n",
        "    Y = 0\n",
        "    for i, (imgs, masks) in enumerate(dataloader):\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        preds = model(imgs)\n",
        "\n",
        "        loss = loss_fn(preds,masks)\n",
        "\n",
        "        track_loss += loss.item()\n",
        "\n",
        "        predclass = torch.argmax(preds,dim=1)\n",
        "\n",
        "        Y += predclass.sum().item()\n",
        "        X += masks.sum().item()\n",
        "\n",
        "\n",
        "        predclass[predclass==0] = 2\n",
        "\n",
        "        XintY += (predclass==masks).type(torch.float).sum().item()\n",
        "\n",
        "        print(\"Trainig Batch\",i+1,\":\",\"2*XintY:\",2*XintY,\"X:\",X,\"Y:\",Y, \"X+Y:\",X+Y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss = round(track_loss/(i+1),2)\n",
        "        running_dice_coef = round(((2*XintY)/(X+Y)),2)\n",
        "\n",
        "        print(\"Training Batch\", i+1,\":\",\"/\",len(dataloader), \"Running Loss:\",running_loss, \"Running Dice_Coef:\",running_dice_coef)\n",
        "\n",
        "    epoch_loss = running_loss\n",
        "    epoch_dice_coef = running_dice_coef\n",
        "    return epoch_loss, epoch_dice_coef\n",
        "\n",
        "\n",
        "def validation_one_epoch(dataloader, model,loss_fn):\n",
        "    model.eval()\n",
        "    track_loss = 0\n",
        "    XintY = 0\n",
        "    X = 0\n",
        "    Y = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, masks) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            preds = model(imgs)\n",
        "\n",
        "            loss = loss_fn(preds,masks)\n",
        "\n",
        "            track_loss += loss.item()\n",
        "\n",
        "            predclass = torch.argmax(preds,dim=1)\n",
        "\n",
        "            Y += predclass.sum().item()\n",
        "            X += masks.sum().item()\n",
        "\n",
        "            predclass[predclass==0] = 2\n",
        "\n",
        "            XintY += (predclass==masks).type(torch.float).sum().item()\n",
        "\n",
        "            print(\"Validation Batch\",i+1,\":\",\"2*XintY:\",2*XintY,\"X:\",X,\"Y:\",Y, \"X+Y:\",X+Y)\n",
        "\n",
        "\n",
        "            running_loss = round(track_loss/(i+1),2)\n",
        "            running_dice_coef = round(((2*XintY)/(X+Y)),2)\n",
        "\n",
        "            print(\"Validation Batch\", i+1,\":\",\"/\",len(dataloader), \"Running Loss:\",running_loss, \"Running Dice_Coef:\",running_dice_coef)\n",
        "\n",
        "    epoch_loss = running_loss\n",
        "    epoch_dice_coef = running_dice_coef\n",
        "    return epoch_loss, epoch_dice_coef"
      ],
      "metadata": {
        "id": "bKoVEjHx6OMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_network = vgg19_bn(weights = VGG19_BN_Weights.DEFAULT)\n",
        "\n",
        "for param in pretrained_network.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "encoder = Encoder(pretrained_network).to(device)\n",
        "center = Center().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "\n",
        "model = UNET(encoder,center, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
        "epochs = 60\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"Epoch No:\",i+1)\n",
        "    train_epoch_loss, train_epoch_dice_coef=train_one_epoch(train_dataloader,model,loss_fn,optimizer)\n",
        "    print(\"Training Epoch Loss:\", train_epoch_loss, \"Training Epoch Dice_Coef:\", train_epoch_dice_coef)\n",
        "    val_epoch_loss, val_epoch_dice_coef = validation_one_epoch(validation_dataloader,model,loss_fn)\n",
        "    print(\"Validation Epoch Loss:\", val_epoch_loss, \"Validation Epoch Dice_Coef:\", val_epoch_dice_coef)\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "\n",
        "for param in pretrained_network.features.parameters():\n",
        "    param.requires_grad=True\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"Epoch No:\",i+1)\n",
        "    train_epoch_loss, train_epoch_dice_coef=train_one_epoch(train_dataloader,model,loss_fn,optimizer)\n",
        "    print(\"Training Epoch Loss:\", train_epoch_loss, \"Training Epoch Dice_Coef:\", train_epoch_dice_coef)\n",
        "    val_epoch_loss, val_epoch_dice_coef=validation_one_epoch(validation_dataloader,model,loss_fn)\n",
        "    print(\"Validation Epoch Loss:\", val_epoch_loss, \"Validation Epoch Dice_Coef:\", val_epoch_dice_coef)\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "9Q_iUPQz6QZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinalTrainDataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        _,_,self.files = next(os.walk(path))\n",
        "        self.length = int(len(self.files)/2)\n",
        "        self.transform = Compose([ToTensor(), Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx + 21\n",
        "        path = self.path + str(idx) + \"_training.tif\"\n",
        "        img = reshape(path)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        path = self.path + str(idx) + \"_manual1.gif\"\n",
        "        mask = reshape(path)\n",
        "        mask = np.array(mask)\n",
        "        mask = torch.from_numpy(mask).type(torch.long)\n",
        "        mask[mask == 255] = 1\n",
        "\n",
        "        return img,mask"
      ],
      "metadata": {
        "id": "u2e-wxeN6Sim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FinalTrainDataset(\"/kaggle/input/sai-vessel-segmentation2/all/train/\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, True)\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"Epoch No:\",i+1)\n",
        "    train_epoch_loss, train_epoch_dice_coef=train_one_epoch(train_dataloader,model,loss_fn,optimizer)\n",
        "    print(\"Training Epoch Loss:\", train_epoch_loss, \"Training Epoch Dice_Coef:\", train_epoch_dice_coef)\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "8bIQ8nip6VT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        _,_,self.files = next(os.walk(path))\n",
        "        self.length = int(len(self.files))\n",
        "        self.transform = Compose([ToTensor(), Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx + 1\n",
        "        if idx <= 9:\n",
        "            path = self.path + \"0\" + str(idx) + \"_test.tif\"\n",
        "        else:\n",
        "            path = self.path + str(idx) + \"_test.tif\"\n",
        "\n",
        "        img = reshape(path)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "tvAOmXst6XYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TestDataset(\"/kaggle/input/sai-vessel-segmentation2/all/test/\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 2)"
      ],
      "metadata": {
        "id": "PyKsE0bn6aCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_one_epoch(dataloader, model):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(preds.shape[0]):\n",
        "                pred = preds[i,:,:,:]\n",
        "                pred = torch.argmax(pred,dim = 0)\n",
        "                predf = pred.flatten()\n",
        "                pixelidx = np.where(predf==1)[0]+1\n",
        "\n",
        "                run_lengths = []\n",
        "\n",
        "                for pxid in pixelidx:\n",
        "                    if len(run_lengths) == 0:\n",
        "                        run_lengths.extend((pxid,1))\n",
        "                    elif pxid > prev+1:\n",
        "                        run_lengths.extend((pxid,1))\n",
        "                    else:\n",
        "                        run_lengths[-1] += 1\n",
        "                    prev=pxid\n",
        "\n",
        "                output = ' '.join([str(r) for r in run_lengths])\n",
        "\n",
        "                outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "outputs = eval_one_epoch(test_dataloader,model)\n",
        "df = pd.DataFrame(columns=['Id','Predicted'])\n",
        "df['Id'] = [str(i) for i in range(20)]\n",
        "df['Predicted'] = outputs\n",
        "df.to_csv(\"submission.csv\", index=None)\n",
        "df"
      ],
      "metadata": {
        "id": "SAm_Ouj36ag4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}